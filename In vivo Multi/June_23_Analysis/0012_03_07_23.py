# -*- coding: utf-8 -*-
"""
Created on Thu Jun  1 10:36:00 2023

@author: MOCAP
"""


#%% Functions and imports
"""
Import and define functions
"""

import spikeinterface as si


import sys, struct, math, os, time
import numpy as np

sys.path.append(r"C:\Users\Gil\Documents\GitHub\main\In vivo Multi")

from intanutil.read_header import read_header
from intanutil.get_bytes_per_data_block import get_bytes_per_data_block
from intanutil.read_one_data_block import read_one_data_block
from intanutil.notch_filter import notch_filter
from intanutil.data_to_result import data_to_result

import matplotlib.pyplot as plt


from neo.core import SpikeTrain

from quantities import ms, s, Hz
from elephant.spike_train_generation import homogeneous_poisson_process, homogeneous_gamma_process
from elephant.statistics import mean_firing_rate

from elephant.statistics import time_histogram, instantaneous_rate
from elephant.kernels import GaussianKernel

import pandas as pd

import statistics as stat

def read_data(filename):
    """Reads Intan Technologies RHD2000 data file generated by evaluation board GUI.
    
    Data are returned in a dictionary, for future extensibility.
    """

    tic = time.time()
    fid = open(filename, 'rb')
    filesize = os.path.getsize(filename)

    header = read_header(fid)

    print('Found {} amplifier channel{}.'.format(header['num_amplifier_channels'], plural(header['num_amplifier_channels'])))
    print('Found {} auxiliary input channel{}.'.format(header['num_aux_input_channels'], plural(header['num_aux_input_channels'])))
    print('Found {} supply voltage channel{}.'.format(header['num_supply_voltage_channels'], plural(header['num_supply_voltage_channels'])))
    print('Found {} board ADC channel{}.'.format(header['num_board_adc_channels'], plural(header['num_board_adc_channels'])))
    print('Found {} board digital input channel{}.'.format(header['num_board_dig_in_channels'], plural(header['num_board_dig_in_channels'])))
    print('Found {} board digital output channel{}.'.format(header['num_board_dig_out_channels'], plural(header['num_board_dig_out_channels'])))
    print('Found {} temperature sensors channel{}.'.format(header['num_temp_sensor_channels'], plural(header['num_temp_sensor_channels'])))
    print('')

    # Determine how many samples the data file contains.
    bytes_per_block = get_bytes_per_data_block(header)

    # How many data blocks remain in this file?
    data_present = False
    bytes_remaining = filesize - fid.tell()
    if bytes_remaining > 0:
        data_present = True

    if bytes_remaining % bytes_per_block != 0:
        raise Exception('Something is wrong with file size : should have a whole number of data blocks')

    num_data_blocks = int(bytes_remaining / bytes_per_block)

    num_amplifier_samples = header['num_samples_per_data_block'] * num_data_blocks
    num_aux_input_samples = int((header['num_samples_per_data_block'] / 4) * num_data_blocks)
    num_supply_voltage_samples = 1 * num_data_blocks
    num_board_adc_samples = header['num_samples_per_data_block'] * num_data_blocks
    num_board_dig_in_samples = header['num_samples_per_data_block'] * num_data_blocks
    num_board_dig_out_samples = header['num_samples_per_data_block'] * num_data_blocks

    record_time = num_amplifier_samples / header['sample_rate']

    if data_present:
        print('File contains {:0.3f} seconds of data.  Amplifiers were sampled at {:0.2f} kS/s.'.format(record_time, header['sample_rate'] / 1000))
    else:
        print('Header file contains no data.  Amplifiers were sampled at {:0.2f} kS/s.'.format(header['sample_rate'] / 1000))

    if data_present:
        # Pre-allocate memory for data.
        print('')
        print('Allocating memory for data...')

        data = {}
        if (header['version']['major'] == 1 and header['version']['minor'] >= 2) or (header['version']['major'] > 1):
            data['t_amplifier'] = np.zeros(num_amplifier_samples, dtype=np.int_)
        else:
            data['t_amplifier'] = np.zeros(num_amplifier_samples, dtype=np.uint)

        data['amplifier_data'] = np.zeros([header['num_amplifier_channels'], num_amplifier_samples], dtype=np.uint)
        data['aux_input_data'] = np.zeros([header['num_aux_input_channels'], num_aux_input_samples], dtype=np.uint)
        data['supply_voltage_data'] = np.zeros([header['num_supply_voltage_channels'], num_supply_voltage_samples], dtype=np.uint)
        data['temp_sensor_data'] = np.zeros([header['num_temp_sensor_channels'], num_supply_voltage_samples], dtype=np.uint)
        data['board_adc_data'] = np.zeros([header['num_board_adc_channels'], num_board_adc_samples], dtype=np.uint)
        
        # by default, this script interprets digital events (digital inputs and outputs) as booleans
        # if unsigned int values are preferred(0 for False, 1 for True), replace the 'dtype=np.bool_' argument with 'dtype=np.uint' as shown
        # the commented line below illustrates this for digital input data; the same can be done for digital out
        
        #data['board_dig_in_data'] = np.zeros([header['num_board_dig_in_channels'], num_board_dig_in_samples], dtype=np.uint)
        data['board_dig_in_data'] = np.zeros([header['num_board_dig_in_channels'], num_board_dig_in_samples], dtype=np.bool_)
        data['board_dig_in_raw'] = np.zeros(num_board_dig_in_samples, dtype=np.uint)
        
        data['board_dig_out_data'] = np.zeros([header['num_board_dig_out_channels'], num_board_dig_out_samples], dtype=np.bool_)
        data['board_dig_out_raw'] = np.zeros(num_board_dig_out_samples, dtype=np.uint)

        # Read sampled data from file.
        print('Reading data from file...')

        # Initialize indices used in looping
        indices = {}
        indices['amplifier'] = 0
        indices['aux_input'] = 0
        indices['supply_voltage'] = 0
        indices['board_adc'] = 0
        indices['board_dig_in'] = 0
        indices['board_dig_out'] = 0

        print_increment = 10
        percent_done = print_increment
        for i in range(num_data_blocks):
            read_one_data_block(data, header, indices, fid)

            # Increment indices
            indices['amplifier'] += header['num_samples_per_data_block']
            indices['aux_input'] += int(header['num_samples_per_data_block'] / 4)
            indices['supply_voltage'] += 1
            indices['board_adc'] += header['num_samples_per_data_block']
            indices['board_dig_in'] += header['num_samples_per_data_block']
            indices['board_dig_out'] += header['num_samples_per_data_block']            

            fraction_done = 100 * (1.0 * i / num_data_blocks)
            if fraction_done >= percent_done:
                print('{}% done...'.format(percent_done))
                percent_done = percent_done + print_increment

        # Make sure we have read exactly the right amount of data.
        bytes_remaining = filesize - fid.tell()
        if bytes_remaining != 0: raise Exception('Error: End of file not reached.')



    # Close data file.
    fid.close()

    if (data_present):
        print('Parsing data...')

        # Extract digital input channels to separate variables.
        for i in range(header['num_board_dig_in_channels']):
            data['board_dig_in_data'][i, :] = np.not_equal(np.bitwise_and(data['board_dig_in_raw'], (1 << header['board_dig_in_channels'][i]['native_order'])), 0)

        # Extract digital output channels to separate variables.
        for i in range(header['num_board_dig_out_channels']):
            data['board_dig_out_data'][i, :] = np.not_equal(np.bitwise_and(data['board_dig_out_raw'], (1 << header['board_dig_out_channels'][i]['native_order'])), 0)

        # Scale voltage levels appropriately.
        data['amplifier_data'] = np.multiply(0.195, (data['amplifier_data'].astype(np.int32) - 32768))      # units = microvolts
        data['aux_input_data'] = np.multiply(37.4e-6, data['aux_input_data'])               # units = volts
        data['supply_voltage_data'] = np.multiply(74.8e-6, data['supply_voltage_data'])     # units = volts
        if header['eval_board_mode'] == 1:
            data['board_adc_data'] = np.multiply(152.59e-6, (data['board_adc_data'].astype(np.int32) - 32768)) # units = volts
        elif header['eval_board_mode'] == 13:
            data['board_adc_data'] = np.multiply(312.5e-6, (data['board_adc_data'].astype(np.int32) - 32768)) # units = volts
        else:
            data['board_adc_data'] = np.multiply(50.354e-6, data['board_adc_data'])           # units = volts
        data['temp_sensor_data'] = np.multiply(0.01, data['temp_sensor_data'])               # units = deg C

        # Check for gaps in timestamps.
        num_gaps = np.sum(np.not_equal(data['t_amplifier'][1:]-data['t_amplifier'][:-1], 1))
        if num_gaps == 0:
            print('No missing timestamps in data.')
        else:
            print('Warning: {0} gaps in timestamp data found.  Time scale will not be uniform!'.format(num_gaps))

        # Scale time steps (units = seconds).
        data['t_amplifier'] = data['t_amplifier'] / header['sample_rate']
        data['t_aux_input'] = data['t_amplifier'][range(0, len(data['t_amplifier']), 4)]
        data['t_supply_voltage'] = data['t_amplifier'][range(0, len(data['t_amplifier']), header['num_samples_per_data_block'])]
        data['t_board_adc'] = data['t_amplifier']
        data['t_dig'] = data['t_amplifier']
        data['t_temp_sensor'] = data['t_supply_voltage']

        # If the software notch filter was selected during the recording, apply the
        # same notch filter to amplifier data here.
        if header['notch_filter_frequency'] > 0 and header['version']['major'] < 3:
            print('Applying notch filter...')

            print_increment = 10
            percent_done = print_increment
            for i in range(header['num_amplifier_channels']):
                data['amplifier_data'][i,:] = notch_filter(data['amplifier_data'][i,:], header['sample_rate'], header['notch_filter_frequency'], 10)

                fraction_done = 100 * (i / header['num_amplifier_channels'])
                if fraction_done >= percent_done:
                    print('{}% done...'.format(percent_done))
                    percent_done += print_increment
    else:
        data = [];

    # Move variables to result struct.
    result = data_to_result(header, data, data_present)

    print('Done!  Elapsed time: {0:0.1f} seconds'.format(time.time() - tic))
    return result

def plural(n):
    """Utility function to optionally pluralize words based on the value of n.
    """

    if n == 1:
        return ''
    else:
        return 's'


def filter_signal(signal, order=3, sample_rate=20000, freq_low=300, freq_high=6000, axis=0):
    import scipy.signal
    Wn = [freq_low / (sample_rate / 2), freq_high / (sample_rate / 2)]
    sos_coeff = scipy.signal.iirfilter(order, Wn, btype="band", ftype="butter", output="sos")
    filtered_signal = scipy.signal.sosfiltfilt(sos_coeff, signal, axis=axis)
    return filtered_signal


def extract_spike_waveform(signal, spike_idx, left_width=(5/1000)*20000/2, right_width=(5/1000)*20000/2):
    
    '''
    Function to extract spikes waveforms in spike2 recordings
    
    INPUTS :
        signal (1-d array) : the ephy signal
        spike_idx (1-d array or integer list) : array containing the spike indexes (in points)
        width (int) = width for spike window
    
    OUTPUTS : 
        SPIKES (list) : a list containg the waveform of each spike 
    
    '''
    
    SPIKES = []
    
    left_width = int(left_width)
    right_width = int(right_width)
    
    for i in range(len(spike_idx)): 
        index = spike_idx[i]

        spike_wf = signal[index-left_width : index+right_width]

        SPIKES.append(spike_wf)
    return SPIKES


def get_file_names(directory):
    file_names = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            file_path = os.path.join(root, file)
            file_names.append(file_path)
    return file_names



"""
MOCAP
"""
#Load MOCAP class
sys.path.append(r'C:\Users\Gil\Documents\GitHub\main\MOCAP')
import MOCAP_analysis_class as MA


#%% files loading
"""
Load the different files

"""


#Exported from tridesclou export (by spikeinterface to phy)
spike_times = np.load(r'D:/Seafile/Seafile/Data/ePhy/2_SI_data/0012_03_07/phy_export/tridesclous/spike_times.npy')
spike_cluster = np.load(r'D:/Seafile/Seafile/Data/ePhy/2_SI_data/0012_03_07/phy_export/tridesclous/spike_clusters.npy')
spike_templates = np.load(r'D:/Seafile/Seafile/Data/ePhy/2_SI_data/0012_03_07/phy_export/tridesclous/similar_templates.npy')



#List of recordings rhd files
recordings=["D:/Seafile/Seafile/Data/ePhy/1_raw_intan/0012/07_03/0012_07_03_File_01.rhd",
"D:/Seafile/Seafile/Data/ePhy/1_raw_intan/0012/07_03/0012_07_03_File_02.rhd",
"D:/Seafile/Seafile/Data/ePhy/1_raw_intan/0012/07_03/0012_07_03_File_03.rhd",
"D:/Seafile/Seafile/Data/ePhy/1_raw_intan/0012/07_03/0012_07_03_File_05.rhd"]


savefig_folder=r"D:/Seafile/Seafile/Data/ePhy/Output/Spikesorting_0012_03_07"

baseline_duration = 310.9312 #in second




#RHD file reading

multi_recordings,recordings_lengths,multi_stim_idx,multi_frame_idx,frame_start_delay=[],[],[],[],[]

#Concatenate recordings
for record in recordings:
    reader=read_data(record)
    signal = reader['amplifier_data'] 
    recordings_lengths.append(len(signal[0]))
    multi_recordings.append(signal)  
    
    stim_idx=reader['board_dig_in_data'][0]#Digital data for stim of the file
    multi_stim_idx.append(stim_idx)#Digital data for stim of all the files
    
    frame_idx=reader['board_dig_in_data'][1]#Get digital data for mocap ttl
    multi_frame_idx.append(frame_idx)#Digital data for mocap ttl of all the files
    




anaglog_signal_concatenated = np.hstack(multi_recordings)    #Signal concatenated from all the files
digital_stim_signal_concatenated=np.hstack(multi_stim_idx)   #Digital data for stim concatenated from all the files
digital_mocap_signal_concatenated=np.hstack(multi_frame_idx) 

#Get sampling freq
sampling_rate=reader['frequency_parameters']['amplifier_sample_rate']

recordings_lengths_cumsum=np.cumsum(np.array(recordings_lengths)/sampling_rate)

#Get spikes for each cluster 
clusters_idx = np.unique(spike_cluster)#Index of all clusters


clustered_spike_times,clustered_spike_indexes=[],[]
for cluster in clusters_idx:        #Loop on each cluster to get the spike of the cluster
    array_idx = np.where(spike_cluster==cluster)[0]
    selected_spike_idx = np.take(spike_times,array_idx)#Spikes from the cluster
    
    clustered_spike_indexes.append(selected_spike_idx)#All the spikes index by cluster
    clustered_spike_times.append(selected_spike_idx/sampling_rate) #All the spikes times in seconds by cluster

#%%Mocap file loading    

#Mocap files location
mocap_folder=r'D:\Seafile\Seafile\Data\ePhy\3_Mocap_data\0012'

#First Loop : loop on all csv files to list them in the list "Files"
Files = []
for r, d, f in os.walk(mocap_folder):
# r=root, d=directories, f = files
    for filename in f:
        if '.csv' in filename:
            Files.append(os.path.join(r, filename))
            
print('Files to analyze : {}'.format(len(Files)))
i=1


for file in Files:
    #Load csv file
    data_MOCAP = MA.MOCAP_file(file)
    
    #Get Trial, session idexes
    idx = data_MOCAP.whole_idx()    
    
    #Get coords for each foot
    left_foot=data_MOCAP.coord(f"{data_MOCAP.subject()}:Left_Foot")
    right_foot=data_MOCAP.coord(f"{data_MOCAP.subject()}:Right_Foot")    
    
    #Plot trajectory of each foot
    plt.figure()
    plt.title(f'Raw Feet trajectory {data_MOCAP.subject()}_{data_MOCAP.session_idx()}_{data_MOCAP.trial_idx()}')
    plt.plot(-left_foot[1],left_foot[2],color='red',label='Left')
    plt.plot(-right_foot[1],right_foot[2],color='blue',label='Right')
    
    #Plot a vertical line for the IR Beam
    plt.axvline(-stat.median(data_MOCAP.coord(f"{data_MOCAP.subject()}:IR Beam1")[1]))
    
    plt.ylim(0,50)
    
    
    plt.plot(data_MOCAP.calculate_angle(f"{data_MOCAP.subject()}:Left_Foot", f"{data_MOCAP.subject()}:Left_Knee", f"{data_MOCAP.subject()}:Left_Hip"))
    plt.plot(data_MOCAP.calculate_angle(f"{data_MOCAP.subject()}:Right_Foot", f"{data_MOCAP.subject()}:Right_Knee", f"{data_MOCAP.subject()}:Right_Hip"))



    
    A,B,C=np.asarray(data_MOCAP.coord(f"{data_MOCAP.subject()}:Left_Foot"))[1:],np.asarray(data_MOCAP.coord(f"{data_MOCAP.subject()}:Left_Knee"))[1:],np.asarray(data_MOCAP.coord(f"{data_MOCAP.subject()}:Left_Hip"))[1:]
    D,E,F=np.asarray(data_MOCAP.coord(f"{data_MOCAP.subject()}:Right_Foot"))[1:],np.asarray(data_MOCAP.coord(f"{data_MOCAP.subject()}:Right_Knee"))[1:],np.asarray(data_MOCAP.coord(f"{data_MOCAP.subject()}:Right_Hip"))[1:]
    
    
    plt.plot(-A[0],A[1])
    
    
    def calculate_angle(p1, p2, p3):
        # Calcul des vecteurs entre les points
        v1 = p1 - p2
        v2 = p3 - p2
    
        # Calcul de la norme des vecteurs
        norm_v1 = np.linalg.norm(v1, axis=0)
        norm_v2 = np.linalg.norm(v2, axis=0)
    
        # Calcul du produit scalaire entre les vecteurs
        dot_product = np.sum(v1 * v2, axis=0)
    
        # Calcul de l'angle en radians
        angle_rad = np.arccos(dot_product / (norm_v1 * norm_v2))
    
        # Conversion de l'angle en degrés
        angle_deg = np.degrees(angle_rad)
    
        return angle_deg
    
    
    
    def moving_average(data, window_size):
        # Créer une fenêtre de moyenne mobile
        window = np.ones(window_size) / window_size
    
        # Appliquer la moyenne mobile en utilisant la convolution
        smoothed_data = np.convolve(data, window, mode='same')
    
        return smoothed_data
    
    plt.figure()
    angles = calculate_angle(A, B, C)
    angles_averaged = moving_average(angles,50)
    plt.plot(angles_averaged)
    
    angles = calculate_angle(D, E, F)
    angles_averaged = moving_average(angles,50)
    plt.plot(angles_averaged)


#%%Electrode Shape
sites_positions=[[0.0, 250.0],
  [0.0, 300.0],
  [0.0, 350.0],
  [0.0, 200.0],
  [0.0, 150.0],
  [0.0, 100.0],
  [0.0, 50.0],
  [0.0, 0.0],
  [43.3, 25.0],
  [43.3, 75.0],
  [43.3, 125.0],
  [43.3, 175.0],
  [43.3, 225.0],
  [43.3, 275.0],
  [43.3, 325.0],
  [43.3, 375.0]]

channel_order=[12, 13, 14, 15, 11, 10, 9, 8, 7, 6, 5, 4, 0, 1, 2, 3]

channel_positions=list(zip(channel_order,sites_positions))


#%% Vidéo
# for frame_idx in multi_frame_idx:
    
#     frame_ttl=(np.where(frame_idx == True))[0]

#     # Calculer la différence entre les éléments consécutifs
#     diff_indices = np.diff(frame_ttl)

#     # Trouver les indices où la différence n'est pas égale à 1
#     phase_indices = np.where(diff_indices != 1)[0]-1

#     # Extraire le premier index de chaque phase
#     screenshot_idexes = frame_ttl[phase_indices]
    
#     print(screenshot_idexes)

MOCAP_ttl_list=(np.where(digital_mocap_signal_concatenated == True))[0] #Where TTL is true

# Calculer la différence entre les éléments consécutifs
diff_indices = np.diff(MOCAP_ttl_list)
phase_indices = np.where(diff_indices != 1)[0]-1
start_idexes = MOCAP_ttl_list[phase_indices]
start_times=start_idexes/sampling_rate

digital_mocap_signal_concatenated




#%% Filtering preprocessing
"""
Filter signal for each channel
"""

#Set the selected channels for the nalysis during spike sorting
selected_chan=[0,1,2,3,4,5,8,12,13]


filtered_signals=[]
for i in range(len(anaglog_signal_concatenated)):
    if i in selected_chan:
        signal_filtered = filter_signal(anaglog_signal_concatenated[i]) #Filter the signal
        filtered_signals.append(signal_filtered) #Append it in list
        
        
#Whole signal filtered for the selected channel
filtered_signals = np.array(filtered_signals) 

# Calculate the median signal from all filtered signals
median = np.mean(filtered_signals, axis=0)

# Calculate the cmr signals for each channels
cmr_signals = filtered_signals-median     




#%% Figure 1 : Raster plot
print('Figure 1 - Raster plot')
num_events = len(clustered_spike_times)
y_values = list(range(1, num_events + 1))

lineoffsets1 = np.array(y_values)
linelengths1 = [0.1] * num_events

plt.figure()
colors1 = ['C{}'.format(i) for i in range(num_events)]
plt.eventplot(clustered_spike_times, colors=colors1, linelengths=1, lineoffsets=lineoffsets1, linewidths=1)

plt.yticks(y_values)
plt.ylabel('SpikeTrains')  # Nom de l'axe Y
plt.xlabel('Time (s)')  # Nom de l'axe X
plt.title('Figure 1 : Raster plot')
plt.savefig(rf"{savefig_folder}/Figure1_Rasterplot.svg")


#%% Waveforms plotting
"""
Get the waveform of each spiketrain, for each selected channel
"""


for idx,cluster in enumerate(clusters_idx):
    spike_idx=clustered_spike_indexes[idx]#Select spikes for the cluster
    all_mean_wvfs=[]
    for channel in range(len(selected_chan)):
        wvfs = extract_spike_waveform(cmr_signals[channel],spike_idx)#Extract all the wf spikes for the channel
        mean_wvfs = np.mean(wvfs, axis=0)#Compute the mean wf for the channel
        all_mean_wvfs.append(mean_wvfs)#Append to the list for all the channel wf
        
    for chan in range(len(all_mean_wvfs)):
        plt.figure()
        plt.plot(all_mean_wvfs[chan])
        plt.title(rf'Cluster : {cluster} Channel :{selected_chan[chan]}')
        plt.savefig(rf'{savefig_folder}/wf/wf_cluster_{cluster}_chan_{selected_chan[chan]}.png')
        plt.close()

selected_positions = [pos for pos in channel_positions if pos[0] in selected_chan]



waveform_positions = []
for i, t in enumerate(selected_positions):
    new_tuple = t + (all_mean_wvfs[i],)
    waveform_positions.append(new_tuple)

   
"Select channels, or plot everything"

#%% Figure 2 : Elephant Spike Train Analysis
print('Figure 2 - Elephant Spike Train Analysis')
last_spike_time = max(np.concatenate(clustered_spike_times).tolist())

t_stop = last_spike_time+1#stop 1 sec after last spike

elephant_spiketrains = []
for index,spiketrain in enumerate(clustered_spike_times):
    elephant_spiketrains.append(SpikeTrain(spiketrain*s, t_stop=t_stop))



print(rf"There are {len(elephant_spiketrains)} spiketrains in this analysis")
for spiketrain_name,spiketrain in enumerate(elephant_spiketrains):
    
    print(rf"The mean firing rate of spiketrain{spiketrain_name+1} on whole session is", mean_firing_rate(spiketrain))
    
    
    
    plt.figure()    
    
    histogram_count = time_histogram([spiketrain], 0.5*s)
    histogram_rate = time_histogram([spiketrain],  0.5*s, output='rate')
    
    #Histogram info
    """
    print(type(histogram_count), f"of shape {histogram_count.shape}: {histogram_count.shape[0]} samples, {histogram_count.shape[1]} channel")
    print('sampling rate:', histogram_count.sampling_rate)
    print('times:', histogram_count.times)
    print('counts:', histogram_count.T[0])
    print('times:', histogram_rate.times)
    print('rate:', histogram_rate.T[0])
    """   
    
    
    inst_rate = instantaneous_rate(spiketrain, sampling_period=50*ms)
    
    #instantaneous rate info
    """
    print(type(inst_rate), f"of shape {inst_rate.shape}: {inst_rate.shape[0]} samples, {inst_rate.shape[1]} channel")
    print('sampling rate:', inst_rate.sampling_rate)
    print('times (first 10 samples): ', inst_rate.times[:10])
    print('instantaneous rate (first 10 samples):', inst_rate.T[0, :10])
    """

    # plotting the original spiketrain
    # plt.plot(spiketrain, [0]*len(spiketrain), 'r', marker=2, ms=25, markeredgewidth=2, lw=0, label='poisson spike times')
    
    
    # Mean firing rate for the baseline phase
    baseline_stop = baseline_duration
    plt.hlines(mean_firing_rate(spiketrain,t_stop=baseline_stop*s), xmin=spiketrain.t_start, xmax=spiketrain.t_stop, linestyle='--', label='mean firing rate')
    

    
    # time histogram
    plt.bar(histogram_rate.times, histogram_rate.magnitude.flatten(), width=histogram_rate.sampling_period,
            align='edge', alpha=0.3, label='time histogram (rate)',color='black')
    
    
    
    # Instantaneous rate
    plt.plot(inst_rate.times.rescale(s), inst_rate.rescale(histogram_rate.dimensionality).magnitude.flatten(), label='instantaneous rate')
    
    
    
    
    #Length of each recordings
    [plt.axvline(_x, linewidth=1, color='g') for _x in recordings_lengths_cumsum]
    
    [plt.axvline(_x, linewidth=1, color='b') for _x in start_times]
    
    # for i in range(len(starts)):
    #     plt.axvspan(starts[i]-delay, stops[i]-delay, alpha=0.5, color='red')
    # for i in range(len(lifts)):
    #     plt.axvspan(lifts[i], downs[i], alpha=0.5, color='grey')
    
    
    # axis labels and legend
    plt.xlabel('time [{}]'.format(spiketrain.times.dimensionality.latex))
    plt.ylabel('firing rate [{}]'.format(histogram_rate.dimensionality.latex))
    
    plt.xlim(spiketrain.t_start, spiketrain.t_stop)   
    #plt.xlim(0, 572.9232) #Use this to focus on phases you want using recordings_lengths_cumsum
    
    
    plt.legend()
    plt.title(rf'Spiketrain {spiketrain_name+1}')
    plt.show()
    
    # plt.savefig(rf"{savefig_folder}/Figure2_spiketrain_{spiketrain_name+1}.svg")
    
#%%Figure 2b : Superimposed spiketrains
print('Figure 2b - Superimposed spiketrains')
plt.figure()
for spiketrain_name,spiketrain in enumerate(elephant_spiketrains):
        inst_rate = instantaneous_rate(spiketrain, sampling_period=50*ms)
        
        inst_rate_norm= np.divide(inst_rate, max(inst_rate))
        time_axis = np.array(range(len(inst_rate_norm)))/frequency*1000
        
        
        plt.plot(time_axis,inst_rate_norm, label='instantaneous rate')
        #Length of each recordings
        [plt.axvline(_x, linewidth=1, color='g') for _x in recordings_lengths_cumsum]        
        plt.xlim(spiketrain.t_start, spiketrain.t_stop)  
        plt.ylim(0,5)

plt.savefig(rf"{savefig_folder}/Figure2b_superimposed_spiketrains.svg")
        
        
#%%Phase dependent activity (mean_firing_rate)

"""
Waiting period
"""
waiting_times=list(zip(downs,starts))

#Get spike times in the activity period
spiketrains_waiting=[]
for cluster in selected_spike_times:
    spike_times=cluster
    
    # Sélectionner les temps de potentiel d'action dans les phases d'activité
    waiting_spikes = []
    for debut, fin in waiting_times:
        mask = np.logical_and(spike_times >= debut, spike_times <= fin)
        temps_phase = spike_times[mask]
        waiting_spikes.extend(temps_phase)

    # Convertir la liste en array
    waiting_spikes = np.array(waiting_spikes)
    
    spiketrains_waiting.append(waiting_spikes)

mean_waiting=[]    
for index,spiketrain in enumerate(spiketrains_waiting):
    mean_rate=mean_firing_rate(spiketrain)
    print(rf"The mean firing rate of spiketrain{index+2} during waiting activity is", mean_rate)
    mean_waiting.append(mean_rate)


"""
Crossing activity
"""
#Files #3 and #5
#index 1 and 2 here

activity_times=list(zip(starts,stops))

#Get spike times in the activity period
spiketrains_activity=[]
for cluster in selected_spike_times:
    spike_times=cluster
    
    # Sélectionner les temps de potentiel d'action dans les phases d'activité
    activity_spikes = []
    for debut, fin in activity_times:
        mask = np.logical_and(spike_times >= debut, spike_times <= fin)
        temps_phase = spike_times[mask]
        activity_spikes.extend(temps_phase)

    # Convertir la liste en array
    activity_spikes = np.array(activity_spikes)
    
    spiketrains_activity.append(activity_spikes)

mean_activity=[]    
for index,spiketrain in enumerate(spiketrains_activity):
    mean_rate=mean_firing_rate(spiketrain)
    print(rf"The mean firing rate of spiketrain{index+2} during crossing activity is", mean_rate)
    mean_activity.append(mean_rate)



"""
Lifting period
"""
lifting_times=list(zip(starts,stops))

#Get spike times in the activity period
spiketrains_activity=[]
for cluster in selected_spike_times:
    spike_times=cluster
    
    # Sélectionner les temps de potentiel d'action dans les phases d'activité
    activity_spikes = []
    for debut, fin in lifting_times:
        mask = np.logical_and(spike_times >= debut, spike_times <= fin)
        temps_phase = spike_times[mask]
        activity_spikes.extend(temps_phase)

    # Convertir la liste en array
    activity_spikes = np.array(activity_spikes)
    
    spiketrains_activity.append(activity_spikes)

mean_lifting=[]    
for index,spiketrain in enumerate(spiketrains_activity):
    mean_rate=mean_firing_rate(spiketrain)
    print(rf"The mean firing rate of spiketrain{index+2} during lifting activity is", mean_rate)
    mean_lifting.append(mean_rate)


#%%Figure 3 : Histogram mean rate by phase
print('Figure 3 - Histogram mean rate by phase')
means_by_phase=[]
for i,_ in enumerate(clusters_idx):
    means_by_phase.append((mean_waiting[i],mean_activity[i],mean_lifting[i]))


data = means_by_phase
# Nombre de trains et de phases
num_trains = len(data)
num_phases = len(data[0])

# Configuration des paramètres pour le tracé
width = 0.2  # Largeur des barres pour chaque train
x = np.arange(num_trains)  # Position des barres pour chaque phase

# Création de la figure et des sous-graphiques
fig, ax = plt.subplots()

# Parcours de chaque train
for i in range(num_phases):
    # Récupération des valeurs de chaque phase pour le train courant
    values = [data[j][i] for j in range(num_trains)]
    
    # Tracé des barres pour chaque train
    ax.bar(x + i * width, values, width, label=f"phase {i+1}")

# Configuration des axes, des légendes et du titre
ax.set_xticks(x + width * (num_phases - 1) / 2)
ax.set_xticklabels(["train {}".format(i) for i in clusters_idx])
ax.legend()
ax.set_ylabel("Mean rate")
ax.set_title("Mean rate by phase")

# Affichage de la figure
plt.show()
plt.savefig(rf"{savefig_folder}/Figure3_Mean_rates_by_phase.svg")

#%%Figure 3b : Raster activity
print('Figure 3b - Raster activity')

for spiketrain_index,action_potential_times in enumerate(selected_spike_times):
    
    # Durée avant et après la stimulation pour la fenêtre de sélection (en secondes)
    pre_stim_duration = 30
    post_stim_duration = 30
    
    # Calcul de la durée totale
    total_duration = pre_stim_duration + post_stim_duration
    
    # Nombre de bins pour l'histogramme
    num_bins = int(total_duration * 10)  # par exemple, 10 bins par seconde
    
    # Création des limites des bins pour l'histogramme
    bin_edges = np.linspace(-pre_stim_duration, post_stim_duration, num_bins + 1)
    
    # Création du tableau 2D pour stocker les potentiels d'actions sélectionnés
    selected_potentials,selected_potentials_normalized = [],[]
    
    # Parcourir chaque temps de stimulation
    for stim_time in starts:
        # Déterminer les bornes de la fenêtre de sélection
        window_start = stim_time - pre_stim_duration
        window_end = stim_time + post_stim_duration
        
        # Sélectionner les potentiels d'actions dans la fenêtre de sélection
        potentials_in_window = action_potential_times[
            (action_potential_times >= window_start) & (action_potential_times <= window_end)
        ]
        
        
        # Normaliser les temps des potentiels par rapport à la stimulation
        normalized_potentials = potentials_in_window - stim_time
        
        # Ajouter les potentiels d'actions normalisés à selected_potentials
        selected_potentials_normalized.append(normalized_potentials)
    
        
        
        # Ajouter les potentiels d'actions à selected_potentials
        selected_potentials.append(potentials_in_window)
    
    # Convertir selected_potentials en un tableau 2D
    selected_potentials = np.array(selected_potentials)
    
    # Créer la figure et les sous-graphiques
    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios': [2, 1]})
    ax1.set_title(rf"Rasterplot Spiketrain {spiketrain_index+1}")
    
    # Tracer l'histogramme péri-stimulus sur le subplot du bas
    histograms, _ = np.histogram(action_potential_times - starts[:, None], bins=bin_edges)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    ax2.bar(bin_centers, histograms, width=total_duration/num_bins, align='center')
    ax2.set_ylabel('Number of spikes')
    ax2.set_title('Peri-Stimulus Histogram')
    
    
    
    # Tracer le raster plot sur le subplot du haut
    ax1.eventplot(selected_potentials_normalized, lineoffsets=0.5, linelengths=0.5, color='k')
    ax1.set_ylabel('Passage')
    ax1.axvspan(0,0.001,color='blue',alpha=0.3)   
    
    # Normaliser l'axe des x pour le PSTH
    ax2.set_xlim(-pre_stim_duration, post_stim_duration)
    
    # Ajouter des labels pour l'axe des x
    ax2.set_xlabel('Time (s) from start')
    ax2.axvspan(0,0.001,color='blue',alpha=0.3)
    
    # Ajuster les espaces entre les sous-graphiques
    plt.subplots_adjust(hspace=0.3)
    
    # Afficher la figure
    plt.show()
    plt.savefig(rf"{savefig_folder}/Figure3b_Raster_activity_spiketrain_{spiketrain_name+1}.svg")

#%%Figure 4 : Optotag
print('Figure 4 - Optotag')
digital_stim_signal_concatenated_on=(np.where(digital_stim_signal_concatenated == True))[0] #Where the stim is on
stim_starts=np.where(np.diff(digital_stim_signal_concatenated_on)!=1)[0]+1 #The start of each stim
stim_starts=np.insert(stim_starts,0,0)


real_stim_idx=digital_stim_signal_concatenated_on[stim_starts]
real_stim_times=real_stim_idx/frequency

stimulation_times=real_stim_idx/frequency          
    


for spiketrain_index,action_potential_times in enumerate(selected_spike_times):
    
    # Durée avant et après la stimulation pour la fenêtre de sélection (en secondes)
    pre_stim_duration = 0.5
    post_stim_duration = 1
    
    # Calcul de la durée totale
    total_duration = pre_stim_duration + post_stim_duration
    
    # Nombre de bins pour l'histogramme
    num_bins = int(total_duration * 1000)  # par exemple, 10 bins par seconde
    
    # Création des limites des bins pour l'histogramme
    bin_edges = np.linspace(-pre_stim_duration, post_stim_duration, num_bins + 1)
    
    # Création du tableau 2D pour stocker les potentiels d'actions sélectionnés
    selected_potentials,selected_potentials_normalized = [],[]
    
    # Parcourir chaque temps de stimulation
    for stim_time in stimulation_times:
        # Déterminer les bornes de la fenêtre de sélection
        window_start = stim_time - pre_stim_duration
        window_end = stim_time + post_stim_duration
        
        # Sélectionner les potentiels d'actions dans la fenêtre de sélection
        potentials_in_window = action_potential_times[
            (action_potential_times >= window_start) & (action_potential_times <= window_end)
        ]
        
        
        # Normaliser les temps des potentiels par rapport à la stimulation
        normalized_potentials = potentials_in_window - stim_time
        
        # Ajouter les potentiels d'actions normalisés à selected_potentials
        selected_potentials_normalized.append(normalized_potentials)
    
        
        
        # Ajouter les potentiels d'actions à selected_potentials
        selected_potentials.append(potentials_in_window)
    
    # Convertir selected_potentials en un tableau 2D
    selected_potentials = np.array(selected_potentials)
    
    # Créer la figure et les sous-graphiques
    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios': [2, 1]})
    ax1.set_title(rf"Rasterplot Spiketrain {spiketrain_index+2}")
    
    # Tracer l'histogramme péri-stimulus sur le subplot du bas
    histograms, _ = np.histogram(action_potential_times - real_stim_times[:, None], bins=bin_edges)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    ax2.bar(bin_centers, histograms, width=total_duration/num_bins, align='center')
    ax2.set_ylabel('Number of spikes')
    ax2.set_title('Peri-Stimulus Histogram')
    
    
    
    # Tracer le raster plot sur le subplot du haut
    ax1.eventplot(selected_potentials_normalized, lineoffsets=0.5, linelengths=0.5, color='k')
    ax1.set_ylabel('Stimulation')
    ax1.axvspan(0,0.1,color='blue',alpha=0.3)
    
    # Normaliser l'axe des x pour le PSTH
    ax2.set_xlim(-pre_stim_duration, post_stim_duration)
    
    # Ajouter des labels pour l'axe des x
    ax2.set_xlabel('Time (s) from stimulation')
    ax2.axvspan(0,0.1,color='blue',alpha=0.3)
    
    # Ajuster les espaces entre les sous-graphiques
    plt.subplots_adjust(hspace=0.3)
    
    # Afficher la figure
    plt.show()


    plt.savefig(rf"{savefig_folder}/Figure4_optotag_{spiketrain_index+2}.svg")

    first_event_times_normalized = []

    # Parcourir chaque temps de stimulation
    for stim_time in real_stim_times:
        # Trouver les événements qui se produisent après le temps de stimulation
        events_after_stim = action_potential_times[action_potential_times >= stim_time]
        
        if len(events_after_stim) > 0:
            # Le premier événement correspondant est le premier élément après le temps de stimulation
            first_event_time = events_after_stim[0]
            first_event_time_normalized = first_event_time - stim_time
        else:
            # Aucun événement ne correspond à la stimulation
            first_event_time_normalized = np.nan
        
        first_event_times_normalized.append(first_event_time_normalized)

    # Convertir la liste en un tableau numpy
    first_event_times_normalized = np.array(first_event_times_normalized)
    
    # Afficher le graphique
    plt.show()
    
    # Calculer les paramètres de la distribution Gaussienne (moyenne et écart-type)
    mean = np.mean(first_event_times_normalized)
    std = np.std(first_event_times_normalized)
    
    # Créer une plage de valeurs pour la courbe de distribution
    x = np.linspace(np.min(first_event_times_normalized), np.max(first_event_times_normalized), 50)
    
    # Calculer les valeurs de la courbe de distribution Gaussienne
    y = 1 / (std * np.sqrt(2 * np.pi)) * np.exp(-(x - mean)**2 / (2 * std**2))
    
    plt.figure()
    # Tracer l'histogramme
    plt.hist(first_event_times_normalized, bins=300, density=True)  # Utiliser density=True pour normaliser l'histogramme
    
    # Tracer la courbe de distribution
    plt.plot(x, y, color='red', linewidth=2)
    plt.xlim(0,0.2)
    plt.ylim(0,300)
    
    plt.xlabel("Normalized time of apparition (s)")
    plt.ylabel("Probability")
    plt.title(rf"Distribution Histogram - Spiketrain {spiketrain_index+2}")
    
    # Afficher le graphique
    plt.show()
    plt.savefig(rf"{savefig_folder}/Figure4_optotag_distrib_{spiketrain_index+2}.svg")
        

    
